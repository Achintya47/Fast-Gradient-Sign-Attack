# üõ°Ô∏è CNN Adversarial Attack Research
### *Exploring the Fragility of Deep Neural Networks through FGSM*

---

<div align="center">

![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)

**From 98.8% Accuracy To 9.03% Accuracy with Œµ=0.3**

</div>

---

## üìä **Project Overview**

This repository demonstrates the **vulnerability of Convolutional Neural Networks** to adversarial attacks using the **Fast Gradient Sign Method (FGSM)**. What starts as a high-performing MNIST classifier quickly crumbles under carefully crafted perturbations.

> *"The ease with which we can switch between these two worlds is remarkable. In one world, the neural network is confident and accurate. In the other, the same network is confused and wrong."* - Ian Goodfellow

---

## üèóÔ∏è **Model Architecture**

Our CNN follows a **simple yet effective** design philosophy:

```
üì• Input Layer (28√ó28√ó1)
    ‚Üì
üîç Conv2D(1‚Üí8, 3√ó3) + ReLU
    ‚Üì
üìâ MaxPool2D(2√ó2)
    ‚Üì
üîç Conv2D(8‚Üí16, 3√ó3) + ReLU
    ‚Üì
üìâ MaxPool2D(2√ó2)
    ‚Üì
üß† Fully Connected(16√ó7√ó7 ‚Üí 10)
    ‚Üì
üì§ Output (10 classes)
```

### **Implementation Details**
- **Loss Function**: Cross-Entropy Loss
- **Optimizer**: Adam
- **Dataset**: MNIST (28√ó28 grayscale images)
- **Training Accuracy**: **98.8%** ‚ú®

---

## ‚öîÔ∏è **Adversarial Attack: FGSM**

### **What is FGSM?**

The **Fast Gradient Sign Method** is a white-box adversarial attack that generates perturbations by:

1. Computing the gradient of the loss w.r.t. input image
2. Taking the sign of the gradient
3. Scaling by epsilon (Œµ) to control perturbation magnitude

**Mathematical Formula:**
```
x' = x + Œµ √ó sign(‚àá‚Çì J(Œ∏, x, y))
```

Where:
- `x'` = adversarial example
- `x` = original input
- `Œµ` = perturbation magnitude
- `J` = loss function

### **üìö Research Paper**
üîó [**Explaining and Harnessing Adversarial Examples**](https://arxiv.org/abs/1412.6572) - Goodfellow et al., 2014

---

## üî• **Shocking Results**

The model's performance **dramatically degrades** as epsilon increases:

| Epsilon (Œµ) | Test Accuracy | Accuracy Drop | Status |
|-------------|---------------|---------------|--------|
| **0.00** | **98.74%** | - | üü¢ Pristine |
| **0.05** | **93.03%** | ‚Üì 5.71% | üü° Slight Impact |
| **0.10** | **73.87%** | ‚Üì 24.87% | üü† Moderate Impact |
| **0.15** | **45.32%** | ‚Üì 53.42% | üî¥ Severe Impact |
| **0.20** | **25.22%** | ‚Üì 73.52% | üî¥ Critical |
| **0.25** | **14.44%** | ‚Üì 84.30% | üî¥ Near Failure |
| **0.30** | **9.03%** | ‚Üì 89.71% | üî¥ Complete Failure |

### **üìà Visual Impact**

```
Accuracy vs Epsilon
100% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 98.74% (Œµ=0.00)
 90% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   93.03% (Œµ=0.05)
 80% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     73.87% (Œµ=0.10)
 70% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          45.32% (Œµ=0.15)
 60% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               25.22% (Œµ=0.20)
 50% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    14.44% (Œµ=0.25)
 40% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     9.03%  (Œµ=0.30)
 30% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
 20% ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
 10% ‚ñà‚ñà‚ñà‚ñà
  0% ‚ñà‚ñà
```

---

## üöÄ **Key Insights**

> **üîç Critical Finding**: Even with **Œµ = 0.1** (barely perceptible to human eye), the model accuracy drops from **98.8%** to **73.9%** - a catastrophic **24.9%** decrease!

### **Why This Matters**
- **Security Implications**: Real-world AI systems are vulnerable
- **Robustness**: High accuracy ‚â† robust model
- **Adversarial Training**: Need for defensive mechanisms

---

## üîß **Getting Started**

### **Prerequisites**
```bash
pip install torch torchvision numpy matplotlib
```

### **Quick Run**
```bash
git clone https://github.com/Achintya47/Fast-Gradient-Sign-Attack
cd Fast-Gradient-Sign-Attack
jupyter notebook Training_and_Attacking.ipynb
```

---

## üìö **Research Journey**

This project is part of my **summer research exploration** implementing cutting-edge papers in PyTorch. 

### **üîó Other Implementations**
- **[Actor-Critic Algorithm from Scratch](https://github.com/Achintya47/Reinforcement-Learning/tree/main/Actor_Critic_Policy_from_scratch)** - Reinforcement Learning implementation
- *More projects coming soon...*

---

## ü§ù **Connect & Collaborate**

<div align="center">

[![LinkedIn](https://img.shields.io/badge/LinkedIn-%230077B5.svg?style=for-the-badge&logo=linkedin&logoColor=white)](www.linkedin.com/in\achintyasharma47)

**Let's discuss AI security and robustness!**

</div>

---

## üìñ **References**

1. Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). *Explaining and harnessing adversarial examples*. arXiv preprint arXiv:1412.6572.
2. LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). *Gradient-based learning applied to document recognition*. Proceedings of the IEEE.

---

<div align="center">

### *"In adversarial examples, we see the gap between human and machine perception"*

**‚≠ê Star this repo if you found it insightful!**

</div>

---

<sub>**License**: MIT | **Last Updated**: July 2025</sub>
